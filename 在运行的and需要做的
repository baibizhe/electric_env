todo：（1）可以尝试减少神经网络层数,参考stock的激活函数
       （2）调整reward function
       （3） critic 输出层lossfcuntion 可否换成别的（现在为MSE）
       （5） 效果不满意就加减层数。 我觉得很大可能是需要的加层数，别忘了正则化
        (6) 多轮博弈淘汰环境一些发电商， 环境会变
        （7） 给买方设计策略
       -----------------------------------------------------------------------
实验 https://github.com/hongyaozhu98/DDPG-Stock-Trading/blob/master/DDPG.py 需要降级python tf

学习率目前看哪里0.001 0.002是收敛的 ， batchszie  16 128收敛。 gamma没看出来

7.25  神经网络两层，softmax + sigmod  ，  虽然收敛了， 但我怀疑softmax在这里的合理性，所以换成了 relu +sigmoid
8.5   7.25之后停电了几次，电脑的环境出了一些问题，今天配置环境。
      测试了一下原先激活函数有问题， 在几轮过后，actor，critic的梯度都爆炸了。
          尝试（1） 全部换成relu  结果，到爆炸的时间稍长一些
              （2） 全部换成 leakyrealu,激活函数都换leakyrealu  正在运行。 明显不收敛
              （3）换成  softmax + sigmod +tanh      ,critic  softmox +sigmoid
                    critic损失函数换成了 huberloss    https://blog.csdn.net/wfei101/article/details/82531737
8.6 todo：（7） 给买方设计策略  假如这一轮出清价格偏低，下一轮多报电量
    运行结果：  huberloss不收敛,但发现foo_env里面计算出清容量有问题，需要重新设计算法。

8.26 改动： 黄山给agent predict的时候价格梯度的有限值， 并且除以了 10**12 ， 因为平常train到最后 梯度会非常大。






