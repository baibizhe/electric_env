todo：（1）可以尝试减少神经网络层数,参考stock的激活函数
       （2）调整reward function
       （3） critic 输出层lossfcuntion 可否换成别的（现在为MSE）
       （4） 是否需要加 L1,L1 正则化防止过拟合
       （5） 效果不满意就加减层数。 我觉得很大可能是需要的加层数，别忘了正则化

       -----------------------------------------------------------------------


学习率目前看哪里0.001 0.002是收敛的 ， batchszie  16 128收敛。 gamma没看出来

7.25  神经网络两层，softmax + sigmod  ，  虽然收敛了， 但我怀疑softmax在这里的合理性，所以换成了 relu +sigmoid
8.5   7.25之后停电了几次，电脑的环境出了一些问题，今天配置环境。
      测试了一下原先激活函数有问题， 在几轮过后，actor，critic的梯度都爆炸了。
          尝试（1） 全部换成relu  结果，到爆炸的时间稍长一些
              （2） 全部换成 leakyrealu,激活函数都换leakyrealu  正在运行。





