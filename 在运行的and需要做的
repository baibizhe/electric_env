目前第一目标：调整reward function 其他超参数可以推后一下。但还没想好。
-----------------------------------------------------------------------
本电脑在运行原版本的算法，算法外的超参数不一样
大笔记本在运行DDPG加了一层神经网络，神经元数目的=70,50 ， actor隐藏层激活函数全部为relu，
明日可调：（1） critic 输出层lossfcuntion 可否换成别的（现在为MSE）
         （2） 激活函数  可选 SWISH ， Leaky ReLU ， Maxout   https://www.cnblogs.com/makefile/p/activation-function.html
          （3） 是否需要加 L1,L1 正则化防止过拟合
           （4） 效果不满意就加层数。 我觉得很大可能是需要的加层数，别忘了正则化
学习率目前看哪里0.001 0.002是收敛的 ， batchszie  16 128收敛。 gamma没看出来
7.25 神经网络两层，softmax + sigmod  ，  虽然收敛了， 但我怀疑softmax在这里的合理性，所以换成了 relu +sigmoid
8.5 7.25之后停电撒呢次，电脑的环境出了一些问题，今天配置环境。测试了一下， 在几轮过后，actor critic的梯度都爆炸了。
    尝试（1） 全部换成relu  结果，到爆炸的时间稍长一些
        （2） 全部换成 leakyrealu,激活函数都换leakyrealu  正在试验
todo：可以尝试减少神经网络层数,参考stock的激活函数




