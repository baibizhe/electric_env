本电脑在运行原版本的算法，算法外的超参数不一样
大笔记本在运行DDPG加了一层神经网络，神经元数目的=70,50 ， actor隐藏层激活函数全部为relu，
明日可调：（1） critic 输出层lossfcuntion 可否换成别的（现在为MSE）
         （2） 激活函数  可选 SWISH ， Leaky ReLU ， Maxout
          （3） 是否需要加 L1,L1 正则化防止过拟合
           （4） 效果不满意就加层数。 我觉得很大可能是需要的加层数，别忘了正则化
